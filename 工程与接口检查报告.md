# 工程与接口检查报告

## 一、接口与工程检查

### 1. 调用链完整性检查

#### 前端（HTML Demo）→ 后端 → 模型
```
❌ 当前状态：前端与后端完全分离
├─ gpt-demo.html (纯前端 Demo)
│  └─ 使用 mock 数据，无后端连接
│
└─ Android 应用（真实调用链）
   ├─ MainActivity.kt (UI层)
   │  └─ 调用 ModelService.getReply()
   │
   ├─ ModelService.kt (服务层)
   │  └─ 调用 QwenClient.callApi()
   │
   └─ QwenClient.kt (HTTP客户端)
      └─ 调用 DashScope API (真实接口)
```

**结论**：HTML Demo 与 Android 应用是两套独立系统，当前无连接。

---

### 2. 请求入口与结构

#### A. HTML Demo (gpt-demo.html)
- **入口**：`sendMessage()` 函数
- **输入结构**：
  - 文本：`textInput.value.trim()`
  - 图片：`fileInput.files` (FileList)
- **处理逻辑**：**MOCK** - `generateMockResponse()`
- **输出**：前端模拟流式 `streamText()`

#### B. Android 应用
- **入口**：`MainActivity.kt` → `binding.send.setOnClickListener`
- **输入结构**：
  - 文本：`InputNormalizer.normalize(rawInput)` → `String`
  - 图片：**当前不支持**
- **处理链**：
  1. `MainActivity` → `ModelService.getReply()`
  2. `ModelService` → `QwenClient.callApi()`
  3. `QwenClient` → HTTP POST 到 DashScope API
- **输出**：流式回调 `onChunk: (String) -> Unit`

---

### 3. 请求结构详情

#### QwenClient.kt 当前请求格式
```json
{
  "model": "qwen3-vl-flash",
  "input": {
    "prompt": "用户输入的文本"
  },
  "parameters": {
    "temperature": 0.85
  }
}
```

**HTTP Headers**:
- `Authorization: Bearer ${API_KEY}`
- `Content-Type: application/json`

**API Endpoint**:
- `https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation`

---

### 4. 返回结构详情

#### DashScope API 返回格式
```json
{
  "output": {
    "text": "模型生成的文本内容"
  }
}
```

**错误格式**:
```json
{
  "code": "错误码",
  "message": "错误信息"
}
```

#### 当前处理逻辑
- **流式模拟**：将完整响应拆分为 3 字符 chunks，每 50ms 发送一次
- **回调机制**：`onChunk()` 逐块更新，`onComplete()` 完成通知

---

### 5. 功能支持情况

| 功能 | HTML Demo | Android 应用 | 模型能力 |
|------|-----------|--------------|----------|
| **纯文本输入** | ✅ (mock) | ✅ (真实) | ✅ |
| **图片输入（单图）** | ✅ (显示) | ❌ (未实现) | ✅ (qwen3-vl-flash支持) |
| **图片输入（多图）** | ✅ (显示) | ❌ (未实现) | ✅ (qwen3-vl-flash支持) |
| **流式输出** | ✅ (前端模拟) | ✅ (真实流式) | ✅ |
| **历史对话** | ❌ | ❌ | ✅ (需实现) |
| **参数控制** | ❌ | ✅ (temperature=0.85) | ✅ |

---

### 6. 真实接口 vs Mock/Demo

#### ✅ 真实接口
1. **QwenClient.kt** → DashScope API
   - 端点：`https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation`
   - 模型：`qwen3-vl-flash`
   - 认证：Bearer Token (从环境变量 `BAILIAN_API_KEY` 或 `gradle.properties` 读取)
   - 状态：**生产可用**

#### ❌ Mock/Demo/临时逻辑
1. **gpt-demo.html** → `generateMockResponse()`
   - 随机返回预设文本
   - 状态：**纯前端演示**

2. **gpt-demo.html** → `streamText()`
   - 前端 `setInterval` 模拟流式
   - 状态：**UI演示用**

3. **QwenClient.kt** → 流式模拟
   - 将完整响应拆分为 chunks（非真实 SSE/流式）
   - 状态：**临时实现，需改为真实流式**

---

## 二、UI 冻结声明

### 冻结范围
- ✅ **UI 不再新增功能**
- ✅ **不再调整布局、样式、按钮**
- ✅ **所有后续工作默认 UI 不变**

### 当前 UI 状态（已冻结）
- 手机展示框：375px × 812px
- 白色背景消息区
- 白色输入框（深色文字）
- 左侧「+」号按钮（图片上传）
- 右侧黑色圆形发送键（向上箭头）
- 纯文本流式显示（无气泡、无卡片）

---

## 三、核心逻辑工作清单

### A 层：即时输入处理

#### A1. 输入标准化
- [ ] 文本预处理（去除首尾空白、规范化换行）
- [ ] 图片格式验证（支持格式、大小限制）
- [ ] 多图排序与去重
- [ ] 输入长度限制（文本/图片数量）

#### A2. 请求构建
- [ ] 文本输入 → DashScope `input.prompt`
- [ ] 图片输入 → DashScope `input.images[]` (base64)
- [ ] 参数组装（temperature、max_tokens 等）
- [ ] 历史对话上下文（message history）

#### A3. 请求发送
- [ ] HTTP 请求封装（OkHttp）
- [ ] 错误重试机制（网络超时、5xx错误）
- [ ] 请求去重（防止重复提交）

---

### B 层：长期/阶段性摘要

#### B1. 对话历史管理
- [ ] 消息存储结构（本地/内存）
- [ ] 历史窗口大小（token 限制）
- [ ] 自动摘要生成（长对话压缩）
- [ ] 会话持久化（本地数据库）

#### B2. 上下文维护
- [ ] 上下文窗口滑动（保留最近 N 轮）
- [ ] 关键信息锚点提取
- [ ] 上下文压缩策略

---

### 输入解析规则

#### 规则1：文本输入
- 输入：用户输入的原始文本
- 处理：`InputNormalizer.normalize()`
- 输出：标准化后的 `String`
- 约束：最大长度限制（如 4000 tokens）

#### 规则2：图片输入
- 输入：`FileList` (多图)
- 处理：
  1. 格式验证（jpg/png/webp）
  2. 大小限制（单图 < 10MB）
  3. Base64 编码
  4. 添加到 `input.images[]`
- 输出：DashScope 格式的图片数组
- 约束：最多 4 张图片

#### 规则3：混合输入
- 文本 + 图片：同时包含 `input.prompt` 和 `input.images[]`
- 仅文本：只包含 `input.prompt`
- 仅图片：只包含 `input.images[]`（需提供 prompt 占位）

---

### 输出生成规则

#### 规则1：流式输出
- **当前实现**：模拟流式（完整响应后拆分）
- **目标实现**：真实 SSE/流式（逐 token 返回）
- 处理：逐块更新 UI，实时显示

#### 规则2：非流式输出
- 完整响应后一次性显示
- 用于错误处理或降级场景

#### 规则3：输出格式化
- Markdown 渲染（如支持）
- 代码块高亮
- 链接识别

---

### 锚点机制

#### 锚点1：关键信息提取
- 从用户输入中提取关键实体（如作物名称、问题类型）
- 从模型回复中提取关键结论（如建议、步骤）

#### 锚点2：上下文锚定
- 在长对话中标记关键轮次
- 摘要生成时保留锚点信息

#### 锚点3：错误恢复
- 网络错误时保存当前输入状态
- 支持断点续传（重新发送）

---

### 兜底与风险控制

#### 风险1：网络超时
- **兜底**：30s 连接超时，60s 读取超时
- **处理**：显示错误提示，允许重试

#### 风险2：API 限流
- **兜底**：检测 429 状态码
- **处理**：指数退避重试，显示等待提示

#### 风险3：模型不可用
- **兜底**：检测 `ModelUnavailable` 错误码
- **处理**：显示友好错误，建议稍后重试

#### 风险4：响应解析失败
- **兜底**：try-catch 包裹 JSON 解析
- **处理**：打印完整响应，显示解析错误

#### 风险5：内存溢出
- **兜底**：限制历史消息数量（如最近 50 条）
- **处理**：自动清理旧消息，保留摘要

#### 风险6：输入过长
- **兜底**：文本长度限制（4000 tokens）
- **处理**：截断或提示用户缩短输入

---

## 四、下一步工作优先级

### P0（必须）
1. 实现真实流式输出（SSE/流式 HTTP）
2. 实现图片输入支持（单图/多图）
3. 完善错误处理与重试机制

### P1（重要）
4. 实现对话历史管理
5. 实现上下文窗口维护
6. 实现输入标准化与验证

### P2（优化）
7. 实现自动摘要生成
8. 实现锚点机制
9. 实现输出格式化（Markdown）

---

## 五、技术债务

1. **QwenClient.kt** 中的流式模拟需改为真实流式
2. **图片输入** 功能未实现（模型支持但代码未接入）
3. **对话历史** 未实现（每次请求都是独立请求）
4. **HTML Demo** 与 Android 应用完全分离，需统一或明确分工

---

**报告生成时间**：2026-01-26
**状态**：UI 已冻结，进入核心逻辑阶段
